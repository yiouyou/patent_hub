import base64
import gzip
import json
import logging
import os
import pickle
from collections.abc import Callable
from typing import Any, Optional

import frappe
from frappe.model.naming import make_autoname
from frappe.utils import now_datetime, time_diff_in_seconds

# æ—¥å¿—è®¾ç½®
logger = frappe.logger("app.patent_hub.patent_wf._util")
# logger.setLevel(logging.DEBUG)


# ---------------------------------------------------
# ğŸ”¹ æ–‡æœ¬å‹ç¼©ä¸è§£å‹ï¼ˆå­—ç¬¦ä¸² â‡„ base64ï¼‰
# ---------------------------------------------------


def make_json_serializable(obj: Any) -> Any:
	"""
	å°†ä»»æ„å¯¹è±¡è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼
	"""
	if obj is None or isinstance(obj, (str, int, float, bool)):
		return obj
	elif isinstance(obj, bytes):
		# bytes è½¬ä¸ºç‰¹æ®Šæ ‡è®°çš„å­—å…¸
		return {"__type__": "bytes", "__data__": base64.b64encode(obj).decode("ascii")}
	elif isinstance(obj, dict):
		return {str(k): make_json_serializable(v) for k, v in obj.items()}
	elif isinstance(obj, (list, tuple)):
		result = [make_json_serializable(item) for item in obj]
		if isinstance(obj, tuple):
			# ä¿ç•™ tuple ç±»å‹ä¿¡æ¯
			return {"__type__": "tuple", "__data__": result}
		return result
	elif hasattr(obj, "__dict__"):
		# å¤„ç†è‡ªå®šä¹‰å¯¹è±¡
		return {
			"__type__": "object",
			"__class__": obj.__class__.__name__,
			"__data__": make_json_serializable(obj.__dict__),
		}
	else:
		# å…¶ä»–ç±»å‹è½¬ä¸ºå­—ç¬¦ä¸²
		return {"__type__": "str_repr", "__data__": str(obj)}


def restore_from_json_serializable(obj: Any) -> Any:
	"""
	è¿˜åŸ JSON åºåˆ—åŒ–æ—¶è½¬æ¢çš„ç‰¹æ®Šç±»å‹
	"""
	if isinstance(obj, dict):
		if "__type__" in obj:
			if obj["__type__"] == "bytes":
				return base64.b64decode(obj["__data__"].encode("ascii"))
			elif obj["__type__"] == "tuple":
				return tuple(restore_from_json_serializable(item) for item in obj["__data__"])
			elif obj["__type__"] == "str_repr":
				return obj["__data__"]
			elif obj["__type__"] == "object":
				# ç®€å•è¿”å›æ•°æ®éƒ¨åˆ†ï¼Œä¸é‡å»ºå¯¹è±¡
				return restore_from_json_serializable(obj["__data__"])
		else:
			return {k: restore_from_json_serializable(v) for k, v in obj.items()}
	elif isinstance(obj, list):
		return [restore_from_json_serializable(item) for item in obj]
	else:
		return obj


def universal_compress(data: Any) -> str:
	"""
	é€šç”¨å‹ç¼©å‡½æ•°
	æ•°æ®æµ: ä»»æ„æ•°æ® â†’ å­—èŠ‚ â†’ gzipå‹ç¼© â†’ base64ç¼–ç  â†’ å­—ç¬¦ä¸²
	æ”¯æŒæ··åˆç±»å‹çš„å­—å…¸å’Œåˆ—è¡¨
	"""
	# æ­¥éª¤1: è½¬ä¸ºå­—èŠ‚
	if isinstance(data, bytes):
		raw_bytes = data
	elif isinstance(data, str):
		raw_bytes = data.encode("utf-8")
	elif isinstance(data, (dict, list, tuple, int, float, bool)) or data is None:
		try:
			converted_data = make_json_serializable(data)
			json_str = json.dumps(converted_data, ensure_ascii=False, separators=(",", ":"))
			raw_bytes = json_str.encode("utf-8")
		except (TypeError, ValueError):
			# å¦‚æœ JSON åºåˆ—åŒ–ä»ç„¶å¤±è´¥ï¼Œä½¿ç”¨ pickle ä½œä¸ºåå¤‡æ–¹æ¡ˆ
			raw_bytes = pickle.dumps(data)
	else:
		# å¯¹äºå…¶ä»–å¤æ‚ç±»å‹ï¼Œç›´æ¥ä½¿ç”¨ pickle
		raw_bytes = pickle.dumps(data)
	# æ­¥éª¤2: gzipå‹ç¼©
	compressed = gzip.compress(raw_bytes)
	# æ­¥éª¤3: base64ç¼–ç 
	return base64.b64encode(compressed).decode("ascii")


def universal_decompress(compressed_str: str, as_json: bool = False) -> Any:
	"""
	é€šç”¨è§£å‹ç¼©å‡½æ•°
	"""
	try:
		# æ­¥éª¤1: base64è§£ç 
		compressed_bytes = base64.b64decode(compressed_str.encode("ascii"))
		# æ­¥éª¤2: gzipè§£å‹ç¼©
		raw_bytes = gzip.decompress(compressed_bytes)
		if as_json:
			# å°è¯• JSON è§£æ
			try:
				json_str = raw_bytes.decode("utf-8")
				data = json.loads(json_str)
				# è¿˜åŸç‰¹æ®Šç±»å‹
				return restore_from_json_serializable(data)
			except (json.JSONDecodeError, UnicodeDecodeError):
				# JSON è§£æå¤±è´¥ï¼Œä½¿ç”¨ pickle
				return pickle.loads(raw_bytes)
		else:
			# å°è¯•å­—ç¬¦ä¸²è§£ç 
			try:
				return raw_bytes.decode("utf-8")
			except UnicodeDecodeError:
				# ä¸æ˜¯æ–‡æœ¬ï¼Œä½¿ç”¨ pickle
				return pickle.loads(raw_bytes)
	except Exception as e:
		raise ValueError(f"è§£å‹ç¼©å¤±è´¥: {e}")


def text_to_base64(text: str) -> str:
	"""æ–‡æœ¬å­—ç¬¦ä¸²è½¬base64"""
	return base64.b64encode(text.encode("utf-8")).decode("ascii")


def get_attached_files(doc, table_field: str) -> list[dict]:
	"""
	ä»æŒ‡å®šå­è¡¨å­—æ®µä¸­è¯»å– file å­—æ®µï¼Œè½¬æ¢ä¸º base64 å‹ç¼©å­—ç¬¦ä¸²ã€‚
	è¿”å›æ ¼å¼ï¼š[{ file_path: ..., base64: ..., original_filename: ..., note: ... }, ...]
	"""
	results = []
	table = getattr(doc, table_field, [])
	for row in table:
		file_url = row.file
		if not file_url:
			continue
		# åˆ¤æ–­è·¯å¾„ä½ç½®ï¼ˆprivate/publicï¼‰
		if file_url.startswith("/private/files/"):
			filename = file_url.replace("/private/files/", "")
			file_path = os.path.join(frappe.get_site_path("private", "files"), filename)
		elif file_url.startswith("/files/"):
			filename = file_url.replace("/files/", "")
			file_path = os.path.join(frappe.get_site_path("public", "files"), filename)
		else:
			frappe.throw(f"æœªçŸ¥æ–‡ä»¶è·¯å¾„æ ¼å¼: {file_url}")
		if not os.path.exists(file_path):
			frappe.throw(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
		# è·å–åŸå§‹æ–‡ä»¶åï¼ˆåŒ…å«æ‰©å±•åï¼‰
		original_filename = os.path.basename(filename)
		with open(file_path, "rb") as f:
			file_data = f.read()
		results.append({"content_bytes": file_data, "original_filename": original_filename})
	return results


# ---------------------------------------------------
# ğŸ”¹ ç”Ÿæˆæ­¥éª¤å”¯ä¸€ IDï¼ˆåŸºäº patent_id å’Œå‰ç¼€ï¼‰
# ---------------------------------------------------


def generate_step_id(patent_id: str, prefix: str) -> str:
	"""
	ä½¿ç”¨ Frappe çš„ make_autoname ç”Ÿæˆï¼š
	æ ¼å¼ï¼š{patent_id}-{prefix}-001
	"""
	return make_autoname(f"{patent_id}-{prefix}-.#")


# ---------------------------------------------------
# ğŸ”¹ å¿ƒè·³æœºåˆ¶ä¸ç»Ÿä¸€å­—æ®µ
# ---------------------------------------------------

# è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
TASK_TIMEOUTS = {
	"title2scene": 300,
	"info2tech": 300,
	"scene2tech": 300,
	"tech2application": 300,
	"review2revise": 300,
	"align2tex2docx": 300,
	"code2png": 300,
	"md2docx": 300,
}

# å»ºè®®å¿ƒè·³é—´éš”
HEARTBEAT_INTERVAL = 100


def _resolve(doctype_or_doc, name: str | None = None):
	if hasattr(doctype_or_doc, "doctype"):
		return doctype_or_doc.doctype, doctype_or_doc.name
	return doctype_or_doc, name


def update_task_heartbeat(doctype_or_doc, task_key: str, name: str | None = None):
	"""
	âš ï¸ æ— çº¿ç¨‹å†™åº“ã€‚å§‹ç»ˆåœ¨ä»»åŠ¡ï¼ˆé˜Ÿåˆ—ï¼‰ä¸Šä¸‹æ–‡ä¸­è°ƒç”¨ã€‚
	"""
	doctype, docname = _resolve(doctype_or_doc, name)
	heartbeat_field = f"{task_key}_last_heartbeat"
	ts = now_datetime()
	try:
		frappe.db.set_value(doctype, docname, heartbeat_field, ts, update_modified=False)
		frappe.db.commit()
		logger.debug(f"[{task_key}] å¿ƒè·³æ›´æ–°: {doctype}.{docname} at {ts}")
	except Exception as e:
		logger.error(f"[{task_key}] å¿ƒè·³å†™å…¥å¤±è´¥: {doctype}.{docname}, é”™è¯¯: {e}")
		# ä¸ä¸­æ–­ä¸»ä»»åŠ¡


def detect_and_reset_stuck_task(task_key: str, label: str, doctype: str, timeout_seconds=None):
	"""
	åŸºäºå¿ƒè·³çš„å¡æ­»æ£€æµ‹ï¼Œé‡ç½®ä¸º Failedï¼Œå¹¶å®æ—¶æ¨é€ _failedã€‚
	"""
	if timeout_seconds is None:
		timeout_seconds = TASK_TIMEOUTS.get(task_key, 300)  # é»˜è®¤5åˆ†é’Ÿ

	started_at_field = f"{task_key}_started_at"
	heartbeat_field = f"{task_key}_last_heartbeat"
	is_running_field = f"is_running_{task_key}"
	is_done_field = f"is_done_{task_key}"
	run_count_field = f"run_count_{task_key}"
	status_field = f"status_{task_key}"

	stuck_docs = frappe.get_all(
		doctype,
		filters={is_running_field: 1, is_done_field: 0},
		fields=["name", started_at_field, heartbeat_field, run_count_field],
	)

	for doc in stuck_docs:
		if doc.get(run_count_field, 0) == 0:
			logger.info(f"[{label}] è·³è¿‡æœªå¯åŠ¨çš„ä»»åŠ¡: {doctype}.{doc.name}")
			continue

		check_time = doc.get(heartbeat_field) or doc.get(started_at_field)
		if not check_time:
			logger.warning(f"[{label}] ä»»åŠ¡ç¼ºå°‘æ—¶é—´æˆ³: {doctype}.{doc.name}")
			continue

		delta = time_diff_in_seconds(now_datetime(), check_time)
		if delta > timeout_seconds:
			_doc = frappe.get_doc(doctype, doc.name)
			setattr(_doc, is_running_field, 0)
			setattr(_doc, status_field, "Failed")

			timeout_type = "å¿ƒè·³" if doc.get(heartbeat_field) else "å¯åŠ¨"
			_doc.append(
				"comments",
				{
					"comment_type": "Comment",
					"content": f"âš ï¸ è‡ªåŠ¨æ£€æµ‹ï¼š{label} {timeout_type}è¶…æ—¶ï¼ˆ{delta}s > {timeout_seconds}sï¼‰ï¼Œä»»åŠ¡å¯èƒ½å·²å¡æ­»ï¼ŒçŠ¶æ€å·²é‡ç½®ä¸º Failedã€‚å»ºè®®å¿ƒè·³é—´éš”: {HEARTBEAT_INTERVAL}s",
				},
			)
			_doc.save()
			frappe.db.commit()
			# âœ… å®æ—¶å¹¿æ’­å¤±è´¥ï¼ˆå¸¦æˆ¿é—´ + after_commitï¼‰
			try:
				frappe.publish_realtime(
					event=f"{task_key}_failed",
					message={
						"docname": _doc.name,
						"doctype": doctype,
						"error": f"{label}{timeout_type}è¶…æ—¶",
						"step": task_key,
					},
					doctype=doctype,
					docname=_doc.name,
					after_commit=True,
				)
			except Exception as e:
				logger.error(f"[{label}] publish_realtime å¤±è´¥: {e}")
			logger.warning(
				f"[{label}] ä»»åŠ¡{timeout_type}è¶…æ—¶è‡ªåŠ¨é‡ç½®: {doctype}.{_doc.name}, è¶…æ—¶: {delta}s > {timeout_seconds}s"
			)


# æŒ‰ DocType åˆ†ç»„çš„ä»»åŠ¡é…ç½®
DOCTYPE_TASKS = {
	"Patent Workflow": [
		("title2scene", "Title2Scene"),
		("info2tech", "Info2Tech"),
		("scene2tech", "Scene2Tech"),
		("tech2application", "Tech2Application"),
		("review2revise", "Review2Revise"),
		("align2tex2docx", "Align2Tex2Docx"),
	],
	"Code2png": [
		("code2png", "Code2png"),
	],
	"Md2docx": [
		("md2docx", "Md2docx"),
	],
}


def detect_and_reset_all_stuck_tasks(doctype: str):
	if doctype not in DOCTYPE_TASKS:
		logger.warning(f"æœªæ‰¾åˆ° DocType '{doctype}' çš„ä»»åŠ¡é…ç½®")
		return
	tasks = DOCTYPE_TASKS[doctype]
	logger.info(f"å¼€å§‹æ£€æµ‹å¡æ­»ä»»åŠ¡ï¼š{doctype}ï¼ˆå…± {len(tasks)} ä¸ªï¼‰...")
	for key, label in tasks:
		detect_and_reset_stuck_task(key, label, doctype)
	logger.info(f"å¡æ­»ä»»åŠ¡æ£€æµ‹å®Œæˆ: {doctype}")


def detect_and_reset_all_stuck_tasks_multi():
	for doctype in DOCTYPE_TASKS.keys():
		try:
			detect_and_reset_all_stuck_tasks(doctype)
		except Exception as e:
			logger.error(f"æ£€æµ‹ {doctype} å¡æ­»ä»»åŠ¡å¤±è´¥: {e}")


# ---------------------------------------------------
# ğŸ”¹ ä»»åŠ¡å­—æ®µï¼ˆåˆå§‹åŒ–/å®Œæˆ/å¤±è´¥ï¼‰
# ---------------------------------------------------


def init_task_fields(doc, task_key: str, prefix: str, logger=logger):
	"""
	åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€å­—æ®µï¼Œå¹¶ç”Ÿæˆ IDã€‚
	- è®¾ç½®ä¸º Running çŠ¶æ€
	- è‹¥é¦–æ¬¡è¿è¡Œï¼Œåˆ™ç”Ÿæˆ ID
	- ç´¯åŠ  run_count
	- åˆå§‹åŒ–å¿ƒè·³æ—¶é—´

	:param doc: æ–‡æ¡£å¯¹è±¡ï¼ˆä»»æ„DocTypeï¼‰
	:param task_key: ä»»åŠ¡é”®å
	:param prefix: IDå‰ç¼€
	:param logger: æ—¥å¿—å¯¹è±¡
	"""
	id_field = f"{task_key}_id"
	started_at_field = f"{task_key}_started_at"
	heartbeat_field = f"{task_key}_last_heartbeat"
	is_running_field = f"is_running_{task_key}"
	is_done_field = f"is_done_{task_key}"
	status_field = f"status_{task_key}"
	run_count_field = f"run_count_{task_key}"

	if hasattr(doc, "patent_id"):
		setattr(doc, id_field, generate_step_id(doc.patent_id, prefix))
	else:
		setattr(doc, id_field, generate_step_id(doc.name, prefix))

	current_time = now_datetime()
	setattr(doc, is_running_field, 1)
	setattr(doc, is_done_field, 0)
	setattr(doc, status_field, "Running")
	setattr(doc, started_at_field, current_time)
	setattr(doc, heartbeat_field, current_time)
	setattr(doc, run_count_field, getattr(doc, run_count_field, 0) + 1)

	heartbeat_timeout = TASK_TIMEOUTS.get(task_key, 300)
	logger.info(
		f"[{task_key}] åˆå§‹åŒ–ä»»åŠ¡: {doc.doctype}.{doc.name}, id={getattr(doc, id_field)}, status=Running, "
		f"run_count={getattr(doc, run_count_field)}, å¿ƒè·³è¶…æ—¶={heartbeat_timeout}s, å»ºè®®å¿ƒè·³é—´éš”={HEARTBEAT_INTERVAL}s"
	)


def complete_task_fields(
	doc, task_key: str, extra_fields: dict = None, logger=logger, push_realtime: bool = True
):
	is_running_field = f"is_running_{task_key}"
	is_done_field = f"is_done_{task_key}"
	status_field = f"status_{task_key}"
	error_field = f"last_{task_key}_error"
	heartbeat_field = f"{task_key}_last_heartbeat"

	setattr(doc, is_running_field, 0)
	setattr(doc, is_done_field, 1)
	setattr(doc, status_field, "Done")
	setattr(doc, error_field, "æˆåŠŸï¼")
	setattr(doc, heartbeat_field, now_datetime())

	success_count_field = f"success_count_{task_key}"
	_success_count = int(getattr(doc, success_count_field, 0) or 0)
	setattr(doc, success_count_field, _success_count + 1)

	if extra_fields:
		for key, value in extra_fields.items():
			setattr(doc, key, value)
			if key.startswith("cost_"):
				total_field = key.replace("cost_", "total_cost_")
				try:
					current_total = float(getattr(doc, total_field, 0) or 0)
					new_value = float(value or 0)
					setattr(doc, total_field, current_total + new_value)
				except (ValueError, TypeError) as e:
					logger.info(f"Error converting cost values: {e}")
					setattr(doc, total_field, float(value or 0))
			if key.startswith("time_s_"):
				total_field = key.replace("time_s_", "total_time_s_")
				try:
					current_total = float(getattr(doc, total_field, 0) or 0)
					new_value = float(value or 0)
					setattr(doc, total_field, current_total + new_value)
				except (ValueError, TypeError) as e:
					logger.info(f"Error converting time values: {e}")
					setattr(doc, total_field, float(value or 0))

	doc.save()
	frappe.db.commit()
	logger.info(
		f"[{task_key}] ä»»åŠ¡å®Œæˆ: {doc.doctype}.{doc.name}, status=Done, success_count={getattr(doc, success_count_field)}"
	)

	if push_realtime:
		try:
			frappe.publish_realtime(
				event=f"{task_key}_done",
				message={"docname": doc.name, "doctype": doc.doctype, "step": task_key},
				doctype=doc.doctype,
				docname=doc.name,
				after_commit=True,
			)
		except Exception as e:
			logger.error(f"[{task_key}] publish_realtime(_done) å¤±è´¥: {e}")


def fail_task_fields(doc, task_key: str, error: str = None, logger=logger, push_realtime: bool = True):
	is_running_field = f"is_running_{task_key}"
	is_done_field = f"is_done_{task_key}"
	status_field = f"status_{task_key}"
	error_field = f"last_{task_key}_error"
	heartbeat_field = f"{task_key}_last_heartbeat"

	setattr(doc, is_running_field, 0)
	setattr(doc, is_done_field, 0)
	setattr(doc, status_field, "Failed")
	setattr(doc, heartbeat_field, now_datetime())

	error_msg = error or "è¿è¡Œå¤±è´¥"
	if hasattr(doc, error_field):
		setattr(doc, error_field, error_msg)

	doc.save()
	frappe.db.commit()
	logger.error(f"[{task_key}] ä»»åŠ¡å¤±è´¥: {doc.doctype}.{doc.name}, error={error_msg}")

	if push_realtime:
		try:
			frappe.publish_realtime(
				event=f"{task_key}_failed",
				message={"docname": doc.name, "doctype": doc.doctype, "error": error_msg, "step": task_key},
				doctype=doc.doctype,
				docname=doc.name,
				after_commit=True,
			)
		except Exception as e:
			logger.error(f"[{task_key}] publish_realtime(_failed) å¤±è´¥: {e}")


@frappe.whitelist()
def cancel_task(docname: str, task_key: str, doctype: str):
	doc = frappe.get_doc(doctype, docname)
	is_running_field = f"is_running_{task_key}"
	if getattr(doc, is_running_field, 0) != 1:
		return {"success": False, "message": "ä»»åŠ¡æœªå¤„äºè¿è¡ŒçŠ¶æ€ï¼Œæ— æ³•å–æ¶ˆ"}

	fail_task_fields(doc, task_key, "ä»»åŠ¡è¢«ç”¨æˆ·å¼ºåˆ¶ç»ˆæ­¢")
	frappe.db.commit()

	# å¹¿æ’­å®æ—¶å¤±è´¥äº‹ä»¶ï¼ˆå®¹é”™å†æ¬¡å‘é€ï¼›å¸¦æˆ¿é—´ + after_commitï¼‰
	try:
		frappe.publish_realtime(
			event=f"{task_key}_failed",
			message={"docname": docname, "doctype": doctype, "error": "ä»»åŠ¡è¢«ç”¨æˆ·å¼ºåˆ¶ç»ˆæ­¢", "step": task_key},
			doctype=doctype,
			docname=docname,
			after_commit=True,
		)
	except Exception as e:
		logger.error(f"[{task_key}] publish_realtime(cancel) å¤±è´¥: {e}")

	return {"success": True, "message": f"{task_key} å·²è¢«ç»ˆæ­¢"}


# ---------------------------------------------------
# ğŸ”¹ é˜Ÿåˆ—åŒ–å·¥å…·ï¼ˆå¼ºçƒˆå»ºè®® whitelisted API ä½¿ç”¨ï¼‰
# ---------------------------------------------------


def enqueue_long_task(
	*,
	doctype: str,
	docname: str,
	task_key: str,
	prefix: str,
	job_method: str | Callable,
	queue: str = "long",
	timeout: int = 3600,
	job_kwargs: dict | None = None,
) -> dict:
	"""
	åœ¨ whitelist å‡½æ•°å†…è°ƒç”¨ï¼š
	    doc = frappe.get_doc(doctype, docname)
	    init_task_fields(doc, task_key, prefix)
	    doc.save(); frappe.db.commit()
	    return enqueue_long_task(...)

	job_methodï¼šå¯ä¼  import path å­—ç¬¦ä¸²æˆ–å¯è°ƒç”¨å¯¹è±¡ï¼ˆçœŸæ­£çš„é•¿é€»è¾‘ï¼‰ã€‚
	"""
	job_kwargs = job_kwargs or {}
	doc = frappe.get_doc(doctype, docname)
	step_id = getattr(doc, f"{task_key}_id")

	frappe.enqueue(
		job_method,
		queue=queue,
		timeout=timeout,
		job_name=step_id,
		doctype=doctype,
		docname=docname,
		task_key=task_key,
		**job_kwargs,
	)
	logger.info(
		f"[{task_key}] å·²å…¥é˜Ÿ: {doctype}.{docname} -> job={step_id}, queue={queue}, timeout={timeout}s"
	)
	return {"ok": True, "queued": True, "job_name": step_id}


# ---------------------------------------------------
# ğŸ”¹ å…¼å®¹æ€§ä¿ç•™ï¼šwith_heartbeatï¼ˆä¸å†èµ·çº¿ç¨‹ï¼‰
# ---------------------------------------------------


def with_heartbeat(task_key: str, doctype: str, heartbeat_interval: int = None):
	"""
	ï¼ˆå…¼å®¹ä¿ç•™ï¼‰ä¸å†èµ·åå°çº¿ç¨‹ã€‚ä»…åšæ—¥å¿—åŒ…è£…ã€‚
	å»ºè®®ï¼šåœ¨é˜Ÿåˆ—ä»»åŠ¡ä¸­æ˜¾å¼è°ƒç”¨ update_task_heartbeat(doc, task_key)ã€‚
	"""

	def decorator(func):
		def wrapper(docname, *args, **kwargs):
			logger.warning(
				f"[{task_key}] with_heartbeat è£…é¥°å™¨å·²åºŸå¼ƒçº¿ç¨‹å®ç°ï¼Œè¯·æ”¹ä¸ºé˜Ÿåˆ—ä»»åŠ¡ä¸­æ˜¾å¼å¿ƒè·³è°ƒç”¨ã€‚"
			)
			return func(docname, *args, **kwargs)

		return wrapper

	return decorator
