# ===============================================
# ğŸ“¦ å·¥å…·æ¨¡å—ï¼špatent_hub.patent_workflow._util
# æè¿°ï¼šæä¾›é€šç”¨ä»»åŠ¡å·¥å…·å‡½æ•°ï¼ŒåŒ…æ‹¬ï¼š
#  - å­—ç¬¦ä¸²å‹ç¼©/è§£å‹
#  - JSON å‹ç¼©/è§£å‹
#  - æ–‡ä»¶å‹ç¼©/è§£å‹
#  - ID ç”Ÿæˆ
#  - å¡æ­»ä»»åŠ¡æ£€æµ‹ä¸é‡ç½®
#  - é€šç”¨ä»»åŠ¡çŠ¶æ€é‡ç½®
# ===============================================

import base64
import gzip
import json
import logging
import os
from typing import Any

import frappe
from frappe.model.naming import make_autoname
from frappe.utils import now_datetime, time_diff_in_seconds

# æ—¥å¿—è®¾ç½®
logger = frappe.logger("app.patent_hub.patent_wf._util")
logger.setLevel(logging.INFO)


# ---------------------------------------------------
# ğŸ”¹ æ–‡æœ¬å‹ç¼©ä¸è§£å‹ï¼ˆå­—ç¬¦ä¸² â‡„ base64ï¼‰
# ---------------------------------------------------


def check_data_type(data: Any) -> str:
	"""æ£€æŸ¥æ•°æ®ç±»å‹"""
	if isinstance(data, str):
		return "string"
	elif isinstance(data, bytes):
		return "bytes"
	elif isinstance(data, (dict, list)):
		return "json"
	else:
		return "other"


def universal_compress(data: Any) -> str:
	"""
	é€šç”¨å‹ç¼©å‡½æ•°
	æ•°æ®æµ: ä»»æ„æ•°æ® â†’ å­—èŠ‚ â†’ gzipå‹ç¼© â†’ base64ç¼–ç  â†’ å­—ç¬¦ä¸²
	"""
	# æ­¥éª¤1: è½¬ä¸ºå­—èŠ‚
	if isinstance(data, bytes):
		raw_bytes = data
	elif isinstance(data, str):
		raw_bytes = data.encode("utf-8")
	elif isinstance(data, (dict, list)):
		json_str = json.dumps(data, ensure_ascii=False, separators=(",", ":"))
		raw_bytes = json_str.encode("utf-8")
	else:
		raise TypeError(f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {type(data)}")
	# æ­¥éª¤2: gzipå‹ç¼©
	compressed = gzip.compress(raw_bytes)
	# æ­¥éª¤3: base64ç¼–ç 
	return base64.b64encode(compressed).decode("ascii")


def universal_decompress(base64_str: str, as_json: bool = False, as_bytes: bool = False) -> str | bytes | Any:
	"""
	é€šç”¨è§£å‹å‡½æ•°
	æ•°æ®æµ: base64å­—ç¬¦ä¸² â†’ gzipå­—èŠ‚ â†’ åŸå§‹å­—èŠ‚ â†’ [å­—ç¬¦ä¸²] â†’ [JSONå¯¹è±¡]
	"""
	# æ­¥éª¤1: base64è§£ç 
	compressed = base64.b64decode(base64_str)
	# æ­¥éª¤2: gzipè§£å‹
	raw_bytes = gzip.decompress(compressed)
	# æ­¥éª¤3: æ ¹æ®éœ€è¦è¿”å›ä¸åŒæ ¼å¼
	if as_bytes:
		return raw_bytes
	# æ­¥éª¤4: UTF-8è§£ç ä¸ºå­—ç¬¦ä¸²
	raw_str = raw_bytes.decode("utf-8")
	# æ­¥éª¤5: JSONè§£æ(å¯é€‰)
	if as_json:
		return json.loads(raw_str)
	return raw_str


def get_attached_files(doc, table_field: str) -> list[dict]:
	"""
	ä»æŒ‡å®šå­è¡¨å­—æ®µä¸­è¯»å– file å­—æ®µï¼Œè½¬æ¢ä¸º base64 å‹ç¼©å­—ç¬¦ä¸²ã€‚
	è¿”å›æ ¼å¼ï¼š[{ file_path: ..., base64: ..., original_filename: ..., note: ... }, ...]
	"""
	results = []
	table = getattr(doc, table_field, [])
	for row in table:
		file_url = row.file
		if not file_url:
			continue
		# åˆ¤æ–­è·¯å¾„ä½ç½®ï¼ˆprivate/publicï¼‰
		if file_url.startswith("/private/files/"):
			filename = file_url.replace("/private/files/", "")
			file_path = os.path.join(frappe.get_site_path("private", "files"), filename)
		elif file_url.startswith("/files/"):
			filename = file_url.replace("/files/", "")
			file_path = os.path.join(frappe.get_site_path("public", "files"), filename)
		else:
			frappe.throw(f"æœªçŸ¥æ–‡ä»¶è·¯å¾„æ ¼å¼: {file_url}")
		if not os.path.exists(file_path):
			frappe.throw(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
		# è·å–åŸå§‹æ–‡ä»¶åï¼ˆåŒ…å«æ‰©å±•åï¼‰
		original_filename = os.path.basename(filename)
		with open(file_path, "rb") as f:
			file_data = f.read()
		results.append(
			{
				"content_bytes": file_data,
				"original_filename": original_filename,
				# "file_path": file_path,
				# "note": row.note,
			}
		)
	return results


def text_to_base64(text: str) -> str:
	"""æ–‡æœ¬å­—ç¬¦ä¸²è½¬base64"""
	return base64.b64encode(text.encode("utf-8")).decode("ascii")


# ---------------------------------------------------
# ğŸ”¹ ç”Ÿæˆæ­¥éª¤å”¯ä¸€ IDï¼ˆåŸºäº patent_id å’Œå‰ç¼€ï¼‰
# ---------------------------------------------------


def generate_step_id(patent_id: str, prefix: str) -> str:
	"""
	ä½¿ç”¨ Frappe çš„ make_autoname ç”Ÿæˆï¼š
	æ ¼å¼ï¼š{patent_id}-{prefix}-001
	"""
	return make_autoname(f"{patent_id}-{prefix}-.#")


# ---------------------------------------------------
# ğŸ”¹ é‡ç½®è¶…æ—¶æœªå®Œæˆä»»åŠ¡çŠ¶æ€ï¼ˆé€šç”¨å‡½æ•°ï¼‰
# ---------------------------------------------------


def detect_and_reset_stuck_task(task_key: str, label: str, timeout_seconds=1800):
	"""
	é€šç”¨å‡½æ•°ï¼šæ£€æµ‹ä»»åŠ¡æ˜¯å¦å¡æ­»ï¼ˆè¶…è¿‡ timeout ç§’æœªå®Œæˆï¼‰ï¼Œå¹¶è‡ªåŠ¨é‡ç½®çŠ¶æ€
	:param task_key: ä»»åŠ¡å­—æ®µå‰ç¼€ï¼ˆå¦‚ align2tex2docxï¼‰
	:param label: ä¸­æ–‡ä»»åŠ¡åç§°ï¼ˆç”¨äºæ—¥å¿—å’Œè¯„è®ºï¼‰
	:param timeout_seconds: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
	"""
	started_at_field = f"{task_key}_started_at"
	is_running_field = f"is_running_{task_key}"
	is_done_field = f"is_done_{task_key}"
	run_count_field = f"run_count_{task_key}"
	status_field = f"status_{task_key}"

	stuck_docs = frappe.get_all(
		"Patent Workflow",
		filters={is_running_field: 1, is_done_field: 0},
		fields=["name", started_at_field, run_count_field],
	)

	for doc in stuck_docs:
		if doc.get(run_count_field, 0) == 0:
			logger.debug(f"[{label}] è·³è¿‡æœªå¯åŠ¨çš„ä»»åŠ¡: {doc.name}")
			continue

		started_at = doc.get(started_at_field)
		if not started_at:
			continue

		delta = time_diff_in_seconds(now_datetime(), started_at)
		if delta > timeout_seconds:
			_doc = frappe.get_doc("Patent Workflow", doc.name)
			setattr(_doc, is_running_field, 0)
			setattr(_doc, status_field, "Failed")
			_doc.append(
				"comments",
				{
					"comment_type": "Comment",
					"content": f"âš ï¸ è‡ªåŠ¨æ£€æµ‹ï¼š{label} è¿è¡Œè¶…æ—¶ï¼ˆ{delta}sï¼‰ï¼ŒçŠ¶æ€å·²é‡ç½®ä¸º Failed",
				},
			)
			_doc.save()
			logger.warning(f"[{label}] ä»»åŠ¡è¶…æ—¶è‡ªåŠ¨é‡ç½®: {_doc.name}")


# ---------------------------------------------------
# ğŸ”¹ æ‰¹é‡ä»»åŠ¡å­—æ®µæ˜ å°„ï¼ˆç»Ÿä¸€å¤„ç†å¤šä¸ªä»»åŠ¡ï¼‰
# ---------------------------------------------------

TASKS = [
	("title2scene", "Title2Scene"),
	("info2tech", "Info2Tech"),
	("scene2tech", "Scene2Tech"),
	("tech2application", "Tech2Application"),
	("review2revise", "Review2Revise"),
	("align2tex2docx", "Align2Tex2Docx"),
]


def detect_and_reset_all_stuck_tasks():
	"""
	æ‰¹é‡æ£€æµ‹æ‰€æœ‰ä»»åŠ¡ï¼Œæ˜¯å¦å­˜åœ¨è¶…æ—¶æœªå®Œæˆçš„çŠ¶æ€ï¼Œå¹¶è‡ªåŠ¨å¤„ç†
	"""
	for key, label in TASKS:
		detect_and_reset_stuck_task(key, label)


# ---------------------------------------------------
# ğŸ”¹ task ç›¸å…³å·¥å…·
# ---------------------------------------------------


def init_task_fields(doc, task_key: str, prefix: str, logger=None):
	"""
	åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€å­—æ®µï¼Œå¹¶ç”Ÿæˆ IDã€‚
	- è®¾ç½®ä¸º Running çŠ¶æ€
	- è‹¥é¦–æ¬¡è¿è¡Œï¼Œåˆ™ç”Ÿæˆ ID
	- ç´¯åŠ  run_count
	"""
	id_field = f"{task_key}_id"
	started_at_field = f"{task_key}_started_at"
	is_running_field = f"is_running_{task_key}"
	is_done_field = f"is_done_{task_key}"
	status_field = f"status_{task_key}"
	run_count_field = f"run_count_{task_key}"

	# # è‹¥å°šæœªç”Ÿæˆ IDï¼Œåˆ™ç”Ÿæˆï¼Œç”Ÿæˆåä¸å˜
	# if not getattr(doc, id_field, None):
	# 	setattr(doc, id_field, generate_step_id(doc.patent_id, prefix))

	# æ¯æ¬¡ init éƒ½ç”Ÿæˆæ–°çš„ID
	setattr(doc, id_field, generate_step_id(doc.patent_id, prefix))

	# è®¾ç½®è¿è¡ŒçŠ¶æ€
	setattr(doc, is_running_field, 1)
	setattr(doc, is_done_field, 0)
	setattr(doc, status_field, "Running")
	setattr(doc, started_at_field, now_datetime())

	# ç´¯åŠ è¿è¡Œæ¬¡æ•°
	setattr(doc, run_count_field, getattr(doc, run_count_field, 0) + 1)

	if logger:
		logger.info(
			f"[{task_key}] åˆå§‹åŒ–ä»»åŠ¡: id={getattr(doc, id_field)}, status=Running, run_count={getattr(doc, run_count_field)}"
		)


def complete_task_fields(doc, task_key: str, extra_fields: dict = None):
	"""
	ç»Ÿä¸€å®Œæˆä»»åŠ¡çŠ¶æ€è®¾ç½®ï¼Œå¹¶ç´¯åŠ è¿è¡ŒæˆåŠŸæ¬¡æ•°å’Œç´¯è®¡è€—æ—¶/æˆæœ¬ã€‚
	"""
	is_running_field = f"is_running_{task_key}"
	is_done_field = f"is_done_{task_key}"
	status_field = f"status_{task_key}"
	success_count_field = f"success_count_{task_key}"

	setattr(doc, is_running_field, 0)
	setattr(doc, is_done_field, 1)
	setattr(doc, status_field, "Done")
	setattr(doc, success_count_field, getattr(doc, success_count_field, 0) + 1)

	if extra_fields:
		for key, value in extra_fields.items():
			setattr(doc, key, value)

			# ç´¯è®¡æˆæœ¬
			if key.startswith("cost_"):
				total_field = key.replace("cost_", "total_cost_")
				setattr(doc, total_field, getattr(doc, total_field, 0) + float(value or 0))

			# ç´¯è®¡æ—¶é—´
			if key.startswith("time_s_"):
				total_field = key.replace("time_s_", "total_time_s_")
				setattr(doc, total_field, getattr(doc, total_field, 0) + float(value or 0))

	doc.save()


def fail_task_fields(doc, task_key: str, error: str = None):
	"""
	è®¾ç½®ä»»åŠ¡å¤±è´¥çŠ¶æ€ï¼Œå¹¶è®°å½•é”™è¯¯ä¿¡æ¯ï¼ˆä¸å¢åŠ  success_countï¼‰
	"""
	is_running_field = f"is_running_{task_key}"
	is_done_field = f"is_done_{task_key}"
	status_field = f"status_{task_key}"
	error_field = f"last_{task_key}_error"

	setattr(doc, is_running_field, 0)
	setattr(doc, is_done_field, 0)
	setattr(doc, status_field, "Failed")

	if hasattr(doc, error_field):
		setattr(doc, error_field, error or "è¿è¡Œå¤±è´¥")

	doc.save()


@frappe.whitelist()
def reset_task_status(docname: str, task_key: str):
	"""
	æ‰‹åŠ¨é‡ç½®ä»»åŠ¡çŠ¶æ€ï¼ˆç”¨äºç”¨æˆ·åœ¨ç•Œé¢ç‚¹å‡»é‡ç½®æŒ‰é’®ï¼‰
	- å°†ä»»åŠ¡æ ‡è®°ä¸º Failed
	- å†™å…¥é”™è¯¯å­—æ®µè¯´æ˜æ˜¯ç”¨æˆ·æ“ä½œ
	"""
	doc = frappe.get_doc("Patent Workflow", docname)
	fail_task_fields(doc, task_key, error="ç”¨æˆ·æ‰‹åŠ¨é‡ç½®ä»»åŠ¡çŠ¶æ€")
	frappe.db.commit()
	return {"success": True, "message": f"ä»»åŠ¡ {task_key} çŠ¶æ€å·²é‡ç½®ä¸º Failed"}


@frappe.whitelist()
def cancel_task(docname: str, task_key: str):
	"""
	ç”¨æˆ·å¼ºåˆ¶ç»ˆæ­¢ä»»åŠ¡ï¼ˆå‰ç«¯ç‚¹å‡»å–æ¶ˆæŒ‰é’®è§¦å‘ï¼‰
	"""
	doc = frappe.get_doc("Patent Workflow", docname)
	is_running_field = f"is_running_{task_key}"
	if getattr(doc, is_running_field, 0) != 1:
		return {"success": False, "message": "ä»»åŠ¡æœªå¤„äºè¿è¡ŒçŠ¶æ€ï¼Œæ— æ³•å–æ¶ˆ"}

	fail_task_fields(doc, task_key, "ä»»åŠ¡è¢«ç”¨æˆ·å¼ºåˆ¶ç»ˆæ­¢")
	frappe.db.commit()

	# å¹¿æ’­å®æ—¶å¤±è´¥äº‹ä»¶
	frappe.publish_realtime(f"{task_key}_failed", {"docname": docname, "error": "ä»»åŠ¡è¢«ç”¨æˆ·å¼ºåˆ¶ç»ˆæ­¢"})

	return {"success": True, "message": f"{task_key} å·²è¢«ç»ˆæ­¢"}
