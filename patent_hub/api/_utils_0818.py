# ===============================================
# ğŸ“¦ å·¥å…·æ¨¡å—ï¼špatent_hub.patent_workflow._util
# æè¿°ï¼šæä¾›é€šç”¨ä»»åŠ¡å·¥å…·å‡½æ•°ï¼ŒåŒ…æ‹¬ï¼š
#  - å­—ç¬¦ä¸²å‹ç¼©/è§£å‹
#  - JSON å‹ç¼©/è§£å‹
#  - æ–‡ä»¶å‹ç¼©/è§£å‹
#  - ID ç”Ÿæˆ
#  - å¡æ­»ä»»åŠ¡æ£€æµ‹ä¸é‡ç½®
#  - é€šç”¨ä»»åŠ¡çŠ¶æ€é‡ç½®
# ===============================================

import base64
import gzip
import json
import logging
import os
import pickle
from typing import Any

import frappe
from frappe.model.naming import make_autoname
from frappe.utils import now_datetime, time_diff_in_seconds

# æ—¥å¿—è®¾ç½®
logger = frappe.logger("app.patent_hub.patent_wf._util")
logger.setLevel(logging.DEBUG)


# ---------------------------------------------------
# ğŸ”¹ æ–‡æœ¬å‹ç¼©ä¸è§£å‹ï¼ˆå­—ç¬¦ä¸² â‡„ base64ï¼‰
# ---------------------------------------------------


def make_json_serializable(obj: Any) -> Any:
	"""
	å°†ä»»æ„å¯¹è±¡è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼
	"""
	if obj is None or isinstance(obj, (str, int, float, bool)):
		return obj
	elif isinstance(obj, bytes):
		# bytes è½¬ä¸ºç‰¹æ®Šæ ‡è®°çš„å­—å…¸
		return {"__type__": "bytes", "__data__": base64.b64encode(obj).decode("ascii")}
	elif isinstance(obj, dict):
		return {str(k): make_json_serializable(v) for k, v in obj.items()}
	elif isinstance(obj, (list, tuple)):
		result = [make_json_serializable(item) for item in obj]
		if isinstance(obj, tuple):
			# ä¿ç•™ tuple ç±»å‹ä¿¡æ¯
			return {"__type__": "tuple", "__data__": result}
		return result
	elif hasattr(obj, "__dict__"):
		# å¤„ç†è‡ªå®šä¹‰å¯¹è±¡
		return {
			"__type__": "object",
			"__class__": obj.__class__.__name__,
			"__data__": make_json_serializable(obj.__dict__),
		}
	else:
		# å…¶ä»–ç±»å‹è½¬ä¸ºå­—ç¬¦ä¸²
		return {"__type__": "str_repr", "__data__": str(obj)}


def restore_from_json_serializable(obj: Any) -> Any:
	"""
	è¿˜åŸ JSON åºåˆ—åŒ–æ—¶è½¬æ¢çš„ç‰¹æ®Šç±»å‹
	"""
	if isinstance(obj, dict):
		if "__type__" in obj:
			if obj["__type__"] == "bytes":
				return base64.b64decode(obj["__data__"].encode("ascii"))
			elif obj["__type__"] == "tuple":
				return tuple(restore_from_json_serializable(item) for item in obj["__data__"])
			elif obj["__type__"] == "str_repr":
				return obj["__data__"]  # ä¿æŒä¸ºå­—ç¬¦ä¸²
			elif obj["__type__"] == "object":
				# ç®€å•è¿”å›æ•°æ®éƒ¨åˆ†ï¼Œä¸é‡å»ºå¯¹è±¡
				return restore_from_json_serializable(obj["__data__"])
		else:
			return {k: restore_from_json_serializable(v) for k, v in obj.items()}
	elif isinstance(obj, list):
		return [restore_from_json_serializable(item) for item in obj]
	else:
		return obj


def universal_compress(data: Any) -> str:
	"""
	é€šç”¨å‹ç¼©å‡½æ•°
	æ•°æ®æµ: ä»»æ„æ•°æ® â†’ å­—èŠ‚ â†’ gzipå‹ç¼© â†’ base64ç¼–ç  â†’ å­—ç¬¦ä¸²
	æ”¯æŒæ··åˆç±»å‹çš„å­—å…¸å’Œåˆ—è¡¨
	"""
	# æ­¥éª¤1: è½¬ä¸ºå­—èŠ‚
	if isinstance(data, bytes):
		raw_bytes = data
	elif isinstance(data, str):
		raw_bytes = data.encode("utf-8")
	elif isinstance(data, (dict, list, tuple, int, float, bool)) or data is None:
		try:
			converted_data = make_json_serializable(data)
			json_str = json.dumps(converted_data, ensure_ascii=False, separators=(",", ":"))
			raw_bytes = json_str.encode("utf-8")
		except (TypeError, ValueError):
			# å¦‚æœ JSON åºåˆ—åŒ–ä»ç„¶å¤±è´¥ï¼Œä½¿ç”¨ pickle ä½œä¸ºåå¤‡æ–¹æ¡ˆ
			raw_bytes = pickle.dumps(data)
	else:
		# å¯¹äºå…¶ä»–å¤æ‚ç±»å‹ï¼Œç›´æ¥ä½¿ç”¨ pickle
		raw_bytes = pickle.dumps(data)
	# æ­¥éª¤2: gzipå‹ç¼©
	compressed = gzip.compress(raw_bytes)
	# æ­¥éª¤3: base64ç¼–ç 
	return base64.b64encode(compressed).decode("ascii")


def universal_decompress(compressed_str: str, as_json: bool = False) -> Any:
	"""
	é€šç”¨è§£å‹ç¼©å‡½æ•°
	"""
	try:
		# æ­¥éª¤1: base64è§£ç 
		compressed_bytes = base64.b64decode(compressed_str.encode("ascii"))
		# æ­¥éª¤2: gzipè§£å‹ç¼©
		raw_bytes = gzip.decompress(compressed_bytes)
		if as_json:
			# å°è¯• JSON è§£æ
			try:
				json_str = raw_bytes.decode("utf-8")
				data = json.loads(json_str)
				# è¿˜åŸç‰¹æ®Šç±»å‹
				return restore_from_json_serializable(data)
			except (json.JSONDecodeError, UnicodeDecodeError):
				# JSON è§£æå¤±è´¥ï¼Œä½¿ç”¨ pickle
				return pickle.loads(raw_bytes)
		else:
			# å°è¯•å­—ç¬¦ä¸²è§£ç 
			try:
				return raw_bytes.decode("utf-8")
			except UnicodeDecodeError:
				# ä¸æ˜¯æ–‡æœ¬ï¼Œä½¿ç”¨ pickle
				return pickle.loads(raw_bytes)
	except Exception as e:
		raise ValueError(f"è§£å‹ç¼©å¤±è´¥: {e}")


def text_to_base64(text: str) -> str:
	"""æ–‡æœ¬å­—ç¬¦ä¸²è½¬base64"""
	return base64.b64encode(text.encode("utf-8")).decode("ascii")


def get_attached_files(doc, table_field: str) -> list[dict]:
	"""
	ä»æŒ‡å®šå­è¡¨å­—æ®µä¸­è¯»å– file å­—æ®µï¼Œè½¬æ¢ä¸º base64 å‹ç¼©å­—ç¬¦ä¸²ã€‚
	è¿”å›æ ¼å¼ï¼š[{ file_path: ..., base64: ..., original_filename: ..., note: ... }, ...]
	"""
	results = []
	table = getattr(doc, table_field, [])
	for row in table:
		file_url = row.file
		if not file_url:
			continue
		# åˆ¤æ–­è·¯å¾„ä½ç½®ï¼ˆprivate/publicï¼‰
		if file_url.startswith("/private/files/"):
			filename = file_url.replace("/private/files/", "")
			file_path = os.path.join(frappe.get_site_path("private", "files"), filename)
		elif file_url.startswith("/files/"):
			filename = file_url.replace("/files/", "")
			file_path = os.path.join(frappe.get_site_path("public", "files"), filename)
		else:
			frappe.throw(f"æœªçŸ¥æ–‡ä»¶è·¯å¾„æ ¼å¼: {file_url}")
		if not os.path.exists(file_path):
			frappe.throw(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
		# è·å–åŸå§‹æ–‡ä»¶åï¼ˆåŒ…å«æ‰©å±•åï¼‰
		original_filename = os.path.basename(filename)
		with open(file_path, "rb") as f:
			file_data = f.read()
		results.append(
			{
				"content_bytes": file_data,
				"original_filename": original_filename,
				# "file_path": file_path,
				# "note": row.note,
			}
		)
	return results


# ---------------------------------------------------
# ğŸ”¹ ç”Ÿæˆæ­¥éª¤å”¯ä¸€ IDï¼ˆåŸºäº patent_id å’Œå‰ç¼€ï¼‰
# ---------------------------------------------------


def generate_step_id(patent_id: str, prefix: str) -> str:
	"""
	ä½¿ç”¨ Frappe çš„ make_autoname ç”Ÿæˆï¼š
	æ ¼å¼ï¼š{patent_id}-{prefix}-001
	"""
	return make_autoname(f"{patent_id}-{prefix}-.#")


# ---------------------------------------------------
# ğŸ”¹ é‡ç½®è¶…æ—¶æœªå®Œæˆä»»åŠ¡çŠ¶æ€ï¼ˆé€šç”¨å‡½æ•°ï¼‰- ä¼˜åŒ–ç‰ˆ
# ---------------------------------------------------

# ä¸åŒä»»åŠ¡çš„è¶…æ—¶æ—¶é—´é…ç½®ï¼ˆç§’ï¼‰
TASK_TIMEOUTS = {
	"title2scene": 3600,  # 60åˆ†é’Ÿ
	"info2tech": 3600,  # 60åˆ†é’Ÿ
	"scene2tech": 3600,  # 60åˆ†é’Ÿ
	"tech2application": 2400,  # 40åˆ†é’Ÿ
	"review2revise": 1800,  # 30åˆ†é’Ÿ
	"align2tex2docx": 1200,  # 20åˆ†é’Ÿ
}


def update_task_heartbeat(doc, task_key: str):
	"""
	æ›´æ–°ä»»åŠ¡å¿ƒè·³æ—¶é—´ï¼Œé˜²æ­¢è¢«è¯¯åˆ¤ä¸ºè¶…æ—¶
	:param doc: Patent Workflow æ–‡æ¡£å¯¹è±¡
	:param task_key: ä»»åŠ¡å­—æ®µå‰ç¼€
	"""
	heartbeat_field = f"{task_key}_last_heartbeat"
	setattr(doc, heartbeat_field, now_datetime())
	doc.save()
	logger.debug(f"[{task_key}] æ›´æ–°å¿ƒè·³æ—¶é—´: {doc.name}")


def detect_and_reset_stuck_task(task_key: str, label: str, timeout_seconds=None):
	"""
	é€šç”¨å‡½æ•°ï¼šæ£€æµ‹ä»»åŠ¡æ˜¯å¦å¡æ­»ï¼ˆè¶…è¿‡ timeout ç§’æœªå®Œæˆï¼‰ï¼Œå¹¶è‡ªåŠ¨é‡ç½®çŠ¶æ€
	æ”¯æŒå¿ƒè·³æœºåˆ¶ï¼Œä¼˜å…ˆæ£€æŸ¥å¿ƒè·³æ—¶é—´
	:param task_key: ä»»åŠ¡å­—æ®µå‰ç¼€ï¼ˆå¦‚ align2tex2docxï¼‰
	:param label: ä¸­æ–‡ä»»åŠ¡åç§°ï¼ˆç”¨äºæ—¥å¿—å’Œè¯„è®ºï¼‰
	:param timeout_seconds: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨TASK_TIMEOUTSä¸­çš„é…ç½®
	"""
	# ä½¿ç”¨é…ç½®çš„è¶…æ—¶æ—¶é—´ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨é»˜è®¤å€¼
	if timeout_seconds is None:
		timeout_seconds = TASK_TIMEOUTS.get(task_key, 1800)

	started_at_field = f"{task_key}_started_at"
	heartbeat_field = f"{task_key}_last_heartbeat"
	is_running_field = f"is_running_{task_key}"
	is_done_field = f"is_done_{task_key}"
	run_count_field = f"run_count_{task_key}"
	status_field = f"status_{task_key}"

	stuck_docs = frappe.get_all(
		"Patent Workflow",
		filters={is_running_field: 1, is_done_field: 0},
		fields=["name", started_at_field, heartbeat_field, run_count_field],
	)

	for doc in stuck_docs:
		if doc.get(run_count_field, 0) == 0:
			logger.info(f"[{label}] è·³è¿‡æœªå¯åŠ¨çš„ä»»åŠ¡: {doc.name}")
			continue

		# ä¼˜å…ˆæ£€æŸ¥å¿ƒè·³æ—¶é—´ï¼Œå¦‚æœæ²¡æœ‰å¿ƒè·³åˆ™ä½¿ç”¨å¼€å§‹æ—¶é—´
		check_time = doc.get(heartbeat_field) or doc.get(started_at_field)
		if not check_time:
			logger.warning(f"[{label}] ä»»åŠ¡ç¼ºå°‘æ—¶é—´æˆ³: {doc.name}")
			continue

		delta = time_diff_in_seconds(now_datetime(), check_time)
		if delta > timeout_seconds:
			_doc = frappe.get_doc("Patent Workflow", doc.name)
			setattr(_doc, is_running_field, 0)
			setattr(_doc, status_field, "Failed")

			# åŒºåˆ†æ˜¯å¿ƒè·³è¶…æ—¶è¿˜æ˜¯å¯åŠ¨è¶…æ—¶
			timeout_type = "å¿ƒè·³" if doc.get(heartbeat_field) else "å¯åŠ¨"

			_doc.append(
				"comments",
				{
					"comment_type": "Comment",
					"content": f"âš ï¸ è‡ªåŠ¨æ£€æµ‹ï¼š{label} {timeout_type}è¶…æ—¶ï¼ˆ{delta}s > {timeout_seconds}sï¼‰ï¼ŒçŠ¶æ€å·²é‡ç½®ä¸º Failed",
				},
			)
			_doc.save()
			logger.warning(f"[{label}] ä»»åŠ¡{timeout_type}è¶…æ—¶è‡ªåŠ¨é‡ç½®: {_doc.name}, è¶…æ—¶æ—¶é—´: {delta}s")


# ---------------------------------------------------
# ğŸ”¹ æ‰¹é‡ä»»åŠ¡å­—æ®µæ˜ å°„ï¼ˆç»Ÿä¸€å¤„ç†å¤šä¸ªä»»åŠ¡ï¼‰
# ---------------------------------------------------

TASKS = [
	("title2scene", "Title2Scene"),
	("info2tech", "Info2Tech"),
	("scene2tech", "Scene2Tech"),
	("tech2application", "Tech2Application"),
	("review2revise", "Review2Revise"),
	("align2tex2docx", "Align2Tex2Docx"),
]


def detect_and_reset_all_stuck_tasks():
	"""
	æ‰¹é‡æ£€æµ‹æ‰€æœ‰ä»»åŠ¡ï¼Œæ˜¯å¦å­˜åœ¨è¶…æ—¶æœªå®Œæˆçš„çŠ¶æ€ï¼Œå¹¶è‡ªåŠ¨å¤„ç†
	ä½¿ç”¨å„ä»»åŠ¡é…ç½®çš„è¶…æ—¶æ—¶é—´
	"""
	for key, label in TASKS:
		detect_and_reset_stuck_task(key, label)


# ---------------------------------------------------
# ğŸ”¹ task ç›¸å…³å·¥å…· - ä¼˜åŒ–ç‰ˆ
# ---------------------------------------------------


def init_task_fields(doc, task_key: str, prefix: str, logger=None):
	"""
	åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€å­—æ®µï¼Œå¹¶ç”Ÿæˆ IDã€‚
	- è®¾ç½®ä¸º Running çŠ¶æ€
	- è‹¥é¦–æ¬¡è¿è¡Œï¼Œåˆ™ç”Ÿæˆ ID
	- ç´¯åŠ  run_count
	- åˆå§‹åŒ–å¿ƒè·³æ—¶é—´
	"""
	id_field = f"{task_key}_id"
	started_at_field = f"{task_key}_started_at"
	heartbeat_field = f"{task_key}_last_heartbeat"
	is_running_field = f"is_running_{task_key}"
	is_done_field = f"is_done_{task_key}"
	status_field = f"status_{task_key}"
	run_count_field = f"run_count_{task_key}"

	# æ¯æ¬¡ init éƒ½ç”Ÿæˆæ–°çš„ID
	setattr(doc, id_field, generate_step_id(doc.patent_id, prefix))

	# è®¾ç½®è¿è¡ŒçŠ¶æ€
	current_time = now_datetime()
	setattr(doc, is_running_field, 1)
	setattr(doc, is_done_field, 0)
	setattr(doc, status_field, "Running")
	setattr(doc, started_at_field, current_time)
	setattr(doc, heartbeat_field, current_time)  # åˆå§‹åŒ–å¿ƒè·³æ—¶é—´

	# ç´¯åŠ è¿è¡Œæ¬¡æ•°
	setattr(doc, run_count_field, getattr(doc, run_count_field, 0) + 1)

	if logger:
		logger.info(
			f"[{task_key}] åˆå§‹åŒ–ä»»åŠ¡: id={getattr(doc, id_field)}, status=Running, run_count={getattr(doc, run_count_field)}, timeout={TASK_TIMEOUTS.get(task_key, 1800)}s"
		)


def complete_task_fields(doc, task_key: str, extra_fields: dict = None, logger=None):
	"""
	ç»Ÿä¸€å®Œæˆä»»åŠ¡çŠ¶æ€è®¾ç½®ï¼Œå¹¶ç´¯åŠ è¿è¡ŒæˆåŠŸæ¬¡æ•°å’Œç´¯è®¡è€—æ—¶/æˆæœ¬ã€‚
	"""
	is_running_field = f"is_running_{task_key}"
	is_done_field = f"is_done_{task_key}"
	status_field = f"status_{task_key}"
	error_field = f"last_{task_key}_error"
	heartbeat_field = f"{task_key}_last_heartbeat"

	setattr(doc, is_running_field, 0)
	setattr(doc, is_done_field, 1)
	setattr(doc, status_field, "Done")
	setattr(doc, error_field, "æˆåŠŸï¼")
	setattr(doc, heartbeat_field, now_datetime())  # æœ€åæ›´æ–°å¿ƒè·³æ—¶é—´

	success_count_field = f"success_count_{task_key}"
	_success_count = int(getattr(doc, success_count_field, 0) or 0)
	setattr(doc, success_count_field, _success_count + 1)

	if extra_fields:
		for key, value in extra_fields.items():
			setattr(doc, key, value)
			# ç´¯è®¡æˆæœ¬
			if key.startswith("cost_"):
				total_field = key.replace("cost_", "total_cost_")
				try:
					current_total = float(getattr(doc, total_field, 0) or 0)
					new_value = float(value or 0)
					setattr(doc, total_field, current_total + new_value)
				except (ValueError, TypeError) as e:
					logger.info(f"Error converting cost values: {e}")
					setattr(doc, total_field, float(value or 0))
			# ç´¯è®¡æ—¶é—´
			if key.startswith("time_s_"):
				total_field = key.replace("time_s_", "total_time_s_")
				try:
					current_total = float(getattr(doc, total_field, 0) or 0)
					new_value = float(value or 0)
					setattr(doc, total_field, current_total + new_value)
				except (ValueError, TypeError) as e:
					if logger:
						logger.info(f"Error converting time values: {e}")
					setattr(doc, total_field, float(value or 0))

	doc.save()
	if logger:
		logger.info(f"[{task_key}] ä»»åŠ¡å®Œæˆ: status=Done, success_count={getattr(doc, success_count_field)}")


def fail_task_fields(doc, task_key: str, error: str = None, logger=None):
	"""
	è®¾ç½®ä»»åŠ¡å¤±è´¥çŠ¶æ€ï¼Œå¹¶è®°å½•é”™è¯¯ä¿¡æ¯ï¼ˆä¸å¢åŠ  success_countï¼‰
	"""
	is_running_field = f"is_running_{task_key}"
	is_done_field = f"is_done_{task_key}"
	status_field = f"status_{task_key}"
	error_field = f"last_{task_key}_error"
	heartbeat_field = f"{task_key}_last_heartbeat"

	setattr(doc, is_running_field, 0)
	setattr(doc, is_done_field, 0)
	setattr(doc, status_field, "Failed")
	setattr(doc, heartbeat_field, now_datetime())  # æ›´æ–°å¿ƒè·³æ—¶é—´

	error_msg = error or "è¿è¡Œå¤±è´¥"
	if hasattr(doc, error_field):
		setattr(doc, error_field, error_msg)

	doc.save()
	if logger:
		logger.error(f"[{task_key}] ä»»åŠ¡å¤±è´¥: error={error_msg}")


@frappe.whitelist()
def reset_task_status(docname: str, task_key: str):
	"""
	æ‰‹åŠ¨é‡ç½®ä»»åŠ¡çŠ¶æ€ï¼ˆç”¨äºç”¨æˆ·åœ¨ç•Œé¢ç‚¹å‡»é‡ç½®æŒ‰é’®ï¼‰
	- å°†ä»»åŠ¡æ ‡è®°ä¸º Failed
	- å†™å…¥é”™è¯¯å­—æ®µè¯´æ˜æ˜¯ç”¨æˆ·æ“ä½œ
	"""
	doc = frappe.get_doc("Patent Workflow", docname)
	fail_task_fields(doc, task_key, error="ç”¨æˆ·æ‰‹åŠ¨é‡ç½®ä»»åŠ¡çŠ¶æ€")
	frappe.db.commit()
	return {"success": True, "message": f"ä»»åŠ¡ {task_key} çŠ¶æ€å·²é‡ç½®ä¸º Failed"}


@frappe.whitelist()
def cancel_task(docname: str, task_key: str):
	"""
	ç”¨æˆ·å¼ºåˆ¶ç»ˆæ­¢ä»»åŠ¡ï¼ˆå‰ç«¯ç‚¹å‡»å–æ¶ˆæŒ‰é’®è§¦å‘ï¼‰
	"""
	doc = frappe.get_doc("Patent Workflow", docname)
	is_running_field = f"is_running_{task_key}"
	if getattr(doc, is_running_field, 0) != 1:
		return {"success": False, "message": "ä»»åŠ¡æœªå¤„äºè¿è¡ŒçŠ¶æ€ï¼Œæ— æ³•å–æ¶ˆ"}

	fail_task_fields(doc, task_key, "ä»»åŠ¡è¢«ç”¨æˆ·å¼ºåˆ¶ç»ˆæ­¢")
	frappe.db.commit()

	# å¹¿æ’­å®æ—¶å¤±è´¥äº‹ä»¶
	frappe.publish_realtime(f"{task_key}_failed", {"docname": docname, "error": "ä»»åŠ¡è¢«ç”¨æˆ·å¼ºåˆ¶ç»ˆæ­¢"})

	return {"success": True, "message": f"{task_key} å·²è¢«ç»ˆæ­¢"}


@frappe.whitelist()
def update_heartbeat(docname: str, task_key: str):
	"""
	æ‰‹åŠ¨æ›´æ–°ä»»åŠ¡å¿ƒè·³æ—¶é—´ï¼ˆä¾›é•¿æ—¶é—´è¿è¡Œçš„APIè°ƒç”¨ï¼‰
	"""
	try:
		doc = frappe.get_doc("Patent Workflow", docname)
		is_running_field = f"is_running_{task_key}"

		# åªæœ‰è¿è¡Œä¸­çš„ä»»åŠ¡æ‰èƒ½æ›´æ–°å¿ƒè·³
		if getattr(doc, is_running_field, 0) != 1:
			return {"success": False, "message": "ä»»åŠ¡æœªå¤„äºè¿è¡ŒçŠ¶æ€"}

		update_task_heartbeat(doc, task_key)
		frappe.db.commit()

		return {"success": True, "message": f"ä»»åŠ¡ {task_key} å¿ƒè·³å·²æ›´æ–°"}
	except Exception as e:
		logger.error(f"æ›´æ–°å¿ƒè·³å¤±è´¥: {e!s}")
		return {"success": False, "message": f"æ›´æ–°å¿ƒè·³å¤±è´¥: {e!s}"}
